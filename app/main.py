import streamlit as st
from pdf_utils import get_pdf_text
from llm_utils import get_vector_store, get_conversational_chain, extract_structured_data
from dashboard_utils import create_evolution_charts

def main():
    """
    Fun√ß√£o principal que executa a aplica√ß√£o Streamlit NutriGenAI.

    A aplica√ß√£o permite o upload de planos alimentares e avalia√ß√µes f√≠sicas em PDF,
    extrai dados estruturados usando IA, exibe um dashboard com a evolu√ß√£o das
    m√©tricas e oferece um chat interativo para tirar d√∫vidas sobre os documentos.
    """
    st.set_page_config(layout="wide", page_title="NutriGenAI", page_icon="ü§ñ")

    # --- INICIALIZA√á√ÉO DO SESSION STATE ---
    # O st.session_state √© usado para manter o estado da aplica√ß√£o entre os reruns.
    # Inicializamos as chaves necess√°rias se elas ainda n√£o existirem.
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = None  # Armazena o banco de dados vetorial para o chat
    if "messages" not in st.session_state:
        st.session_state.messages = []  # Armazena o hist√≥rico de mensagens do chat
    if "structured_data" not in st.session_state:
        st.session_state.structured_data = None  # Armazena os dados extra√≠dos para o dashboard

    st.title("ü§ñ NutriGenAI: An√°lise Inteligente de Planos Alimentares")

    # --- SIDEBAR PARA UPLOAD E PROCESSAMENTO ---
    # A sidebar √© usada para a√ß√µes principais, como o upload de arquivos.
    with st.sidebar:
        st.header("Upload de Arquivos")
        uploaded_files = st.file_uploader(
            "Fa√ßa o upload dos seus arquivos PDF (avalia√ß√£o f√≠sica, plano alimentar):",
            type=["pdf"],
            accept_multiple_files=True
        )

        if uploaded_files:
            if st.button("Processar Arquivos"):
                # 1. Extrair texto bruto dos PDFs
                with st.spinner("Extraindo texto dos PDFs..."):
                    raw_text = get_pdf_text(uploaded_files)
                
                # 2. Extrair dados estruturados (JSON) do texto usando um LLM
                with st.spinner("Analisando documentos com IA para extrair dados..."):
                    st.session_state.structured_data = extract_structured_data(raw_text)

                # 3. Criar um banco de dados vetorial para o chat de perguntas e respostas
                with st.spinner("Criando banco de dados vetorial para o chat..."):
                    st.session_state.vector_store = get_vector_store(raw_text)
                
                # 4. Limpar hist√≥rico de chat anterior e notificar o sucesso
                st.session_state.messages = []
                st.success("Arquivos processados com sucesso!")
                st.rerun() # st.rerun() recarrega a UI para refletir as mudan√ßas de estado

    # --- DASHBOARD DE AN√ÅLISE ---
    st.header("Dashboard de An√°lise")
    if st.session_state.structured_data:
        # Gera e exibe os gr√°ficos de evolu√ß√£o se os dados estiverem dispon√≠veis
        charts = create_evolution_charts(st.session_state.structured_data)
        if charts:
            for chart in charts:
                st.plotly_chart(chart, use_container_width=True)
        else:
            st.warning("N√£o foi poss√≠vel gerar gr√°ficos de evolu√ß√£o com os dados extra√≠dos.")
    else:
        st.warning("Processe seus arquivos para ver o dashboard.")

    # --- SE√á√ÉO DE CHAT ---
    st.header("Chat Interativo")

    # Exibe o hist√≥rico de mensagens
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Captura a nova pergunta do usu√°rio
    if prompt := st.chat_input("Fa√ßa uma pergunta sobre seus documentos..."):
        # Verifica se os documentos foram processados antes de permitir o chat
        if not st.session_state.vector_store:
            st.warning("Por favor, processe os arquivos primeiro.")
            st.stop()

        # Adiciona a pergunta do usu√°rio ao hist√≥rico e √† UI
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Processa a pergunta e obt√©m a resposta do LLM
        with st.chat_message("assistant"):
            with st.spinner("Pensando..."):
                # Busca documentos similares no banco vetorial
                docs = st.session_state.vector_store.similarity_search(prompt)
                # Cria a cadeia de conversa√ß√£o
                chain = get_conversational_chain()
                # Executa a cadeia com os documentos e a pergunta
                response = chain(
                    {"input_documents": docs, "question": prompt},
                    return_only_outputs=True
                )
                response_text = response["output_text"]
                st.markdown(response_text)
        
        # Adiciona a resposta do assistente ao hist√≥rico
        st.session_state.messages.append({"role": "assistant", "content": response_text})

if __name__ == "__main__":
    # Ponto de entrada da aplica√ß√£o
    main()